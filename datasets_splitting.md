# Решение задачи разбиения датасета

## Постановка задачи:

Предполагается что хранение данных будет осуществляться на сетевом диске в виде
чанков по ~10 часов аудио данных. Например для датасета состоящего из 3000 часов
будет 300 zip архивов.

При обучении новой модели мы хотим удобным способом передавать алгоритму обучения
какие чанки из каких датасетов по каким путям достать для обучения.

Планируется что будет очень много разных комбинаций обучения(120 архивов из
`датасет_1` + 40 архивов  из `датасет_2`,  20+50 архивов и т.д.).

Нам было бы удобно, чтобы можно было заранее для N архивов аудиоданных
(те которые по 10 часов) подготовить N feat файлов. Затем, для обучения модели,
в конфиге будут передаваться пути до  N feat файлов, а не до сырых аудио.

При обучении новой акустической модели, данные должны напрямую подгружаться
из N feat файлов, без дополнительной обработки.


## Предлагаемое решение задачи:

Структура конфигурационного файла будет незначительно изменена на следующую:
```json
{
"paths": {
        "comment" : "<<< {откуда_взять,  куда_сформировать_датасет}",
        "means" : "means/location", #use as means for decoder if specified
},
"datasets" : [
  { "audio": "/mount/am_server1/news.tar",  #check read perms
    "data": "/mount/data_server1/news.data", #check write perms
     "role": "train",  "parts": 0.3 },
  { "audio": "none",  #if none not create
    "data": "/mount/data_server1/movies.data",  
    "role": "train",  "parts": 0.1415 }, #many more
],
"stages": {
        "comment" : "<<< какие стадии необходимо запускать (yes/no) >>>",
        "clean"           :  true,  #вместо clean_and_untar
        "prepare utt"     :  true,  #новая старая стадия
        "make lm"         :  true,  #переименовали стадию make_arpa, utt участвует в обогащении.
        "enhance dict"    :  true,  #обогатить и на основе арпы (она сделает текстовичок)
        "make datasets"   :  true,  #на основе обогащ словаря!
        "train dnet"      :  true,
        "compile model"   :  true,
        "test model"      :  true
    }
}
```

#### Формирование датасетов.  
Если стадия make_datasets не пропускается (значение true напротив соотв. поля в stages),
то будет произведена процедура стандартного формирования dataset-файлов. При этом
предполагается следующая процедура формирования имён:  
в конфигурационном файле указаны источник аудио-данных и путь к датасету.   

#### Использование датасетов
Использование датасетов произойдёт на стадии обучения нейронной сети.
Имена использованных датасетов предоставляются в разделах `train_datasets` и
`test_datasets`.

## Ход работы

#### Использование вектора `means`
Значения `means` вычитаются из вектора фичей в процессоре `cms`. Это происходит
при формировании датасета, а также при обработке аудио-данных (в приложении).

#### Переделана секция `datasets`
```json
{
    "datasets" : [
      {
        "audio": "/mount/am_server1/news.tar", #check read perms
        "data": "/mount/data_server1/news.data", #check write perms
        "mean": "/path/to/name.mean", #can be none
         "role": "train",  
         "parts": 0.3
      }
    ]
}
```
Eсли секция `audio` содержит `none`, то обработка корпуса не будет произведена.
Если не присутствует ни одного необработанного файла, а также отсутствует описание
процесса `cms[means]` необходимо завершить дальнейшую обработку корпуса.

Необходимо формировать один вектор `means` для всех датасетов. Как взвешенная
сумма `means` всех датасетов.

#### Существующие файлы

В paths добавлена подсекция ready, задающая расположение готовых файлов.
```json
{
  "ready": {
             "comment" : "<<< Пути к готовым файлам>>>",
             "make lm"         :  "none"
             "train dnet"      :  "none",
             "test model"      :  "none"
         }
}
```


#### Изменения в `corpus_all`
Упрощен метод формирования индивидуальных датасетов. Теперь метод только
формирует поддиректории с конфигами датасета, если найден соответствующий
audio-архив.


#### Формирование общего means
Формирование общего `means` перенeсено в скрипт `means.py`

#### Создание архива и название файлов
Названия генерированных файлов теперь в секции `paths.generated`

#### Зависимые стадии
Появились зависимые стадии
`make utt` не будет пропущена, даже если соотв. стадия указана как `false`,
если включены `make datasets` (для датасетов нужны файлы атрансов),
или если включена `enhance dict` (для обогащения словаре нужны `unknown`)

#### Работа с .labs-файлами
Файл `model_name.count`, формирующийся на основе `labs`-файлов формируется из суммы
`labs` тренировочных датасетов.
 - [x] проверить, что лабс файлы для разных датасетов идентичны  

#### Конспект сделанного в дороге
 - [x] В конфиг добавлены секции `ready_datasets`, `audio_datasets`. Разделенные в одном
месте чтобы не проделывать эту работу 500 раз во всех скриптах.
 - [x] В датасеты добавлена секция `model` где хранится путь к папке с моделью для датасета.
В этой дирректории находится конфиг для датасета.

### Планы
 - [x] Удалить упоминания synonym универсализировать работу с сгенерированными и новыми датасетами  
 - [x] Сгенерить датасеты по нужным путям.  
 - [x] Выдать ошибку для датасетов, для которых существуют аудио и бинарные датасеты.
 - [x] Проверить, что наборы `labs` идентичны для всех датасетов.  
 - [x] Нагенерить тестовых датасетов  
 - [x] Проверить работу на готовых и неготовых датасетах  
 - [x] Проверить, что готовые датасеты складываются правильно.  
 - [x] Провести тестирование готовой модели.
 - [ ] ввести список файлов-зависимостей для каждой стадии и чекать их при выполнении!


### Для документации
Необходимо описать следующие моменты:
- [x] Пример работы с новыми датасетами
- [x] Пример работы с готовыми датасетами
- [x] Смешанный Пример
- [x] Пример с готовыми стадиями
