# Решение задачи разбиения датасета

## Постановка задачи:

Предполагается что хранение данных будет осуществляться на сетевом диске в виде
чанков по ~10 часов аудио данных. Например для датасета состоящего из 3000 часов
будет 300 zip архивов.

При обучении новой модели мы хотим удобным способом передавать алгоритму обучения
какие чанки из каких датасетов по каким путям достать для обучения.

Планируется что будет очень много разных комбинаций обучения(120 архивов из
`датасет_1` + 40 архивов  из `датасет_2`,  20+50 архивов и т.д.).

Нам было бы удобно, чтобы можно было заранее для N архивов аудиоданных
(те которые по 10 часов) подготовить N feat файлов. Затем, для обучения модели,
в конфиге будут передаваться пути до  N feat файлов, а не до сырых аудио.

При обучении новой акустической модели, данные должны напрямую подгружаться
из N feat файлов, без дополнительной обработки.


## Предлагаемое решение задачи:

Структура конфигурационного файла будет незначительно изменена на следующую:
```json
"paths": {
        "comment" : "<<< {откуда_взять,  куда_сформировать_датасет}",
        "corpus_paths" : [
           {"/mount/am_server1/news.tar", "/mount/data_server1/news"}
           "/mount/am_server1/news2.tar", "/mount/data_server1/movies"
           "/mount/am_server_i/news7.tar", "/mount/data_server_k/films"
           "/mount/am_server_i/movies.tar", "/mount/data_server_m/commercials"
        ]
},
"stages": {
        "comment" : "<<< какие стадии необходимо запускать (yes/no) >>>",
        "clean work dir"  :  true,  #вместо clean_and_untar
        "prepare_utt"     :  true,  #новая старая стадия
        "make lm"       :    true,  #переименовали стадию make_arpa, utt участвует в обогащении.
        "enhance dict"    :  true,  #обогатить и на основе арпы (она сделает текстовичок)
        "make datasets"   :  true,  #на основе обогащ словаря!
        "train dnet"      :  true,
        "compile model"   :  true,
        "test model"      :  true
    },
"comment": "<<< путь к датасету, какую его часть берём",
"train datasets" : [
        {"/mount/data_server_i/news/news_j.data", 0.3},
        {"/mount/data_server_k/dataset_m.data", 0.14}
    ],
"test datasets" : [
        { "/mount/data_server_l/news/news1.data", 0.15}
    ]
```

#### Формирование датасетов.  
Если стадия make_datasets не пропускается (значение true напротив соотв. поля в stages),
то будет произведена процедура стандартного формирования dataset-файлов. При этом
предполагается следующая процедура формирования имён:  
в конфигурационном файле указаны источник аудио-данных и путь к датасету.   

#### Использование датасетов
Использование датасетов произойдёт на стадии обучения нейронной сети.
Имена использованных датасетов предоставляются в разделах `train_datasets` и
`test_datasets`.


<div align="center">
<br><br><br>
<a href="index.md">Домой</a>
</div>
