# Планы на ближайшее время

## Новые задачи [27.07-31.08]

- [x] Проверить `multithread_pass` `на Android`
- [x] внести изменения в документацию про андроид (какой sdk/ndk/gradle рекомендуется)
- [x] Проверить `multithread_pass` `на uni_LSTM`
- [ ] По сабвордам: сделать lm на 4, 5 - граммах.
Поправить arpa_parser.py: fro n in range, for o in range ...
Поправить lm_build.sh, стр. 47
Поправить lm_arpa.h, LM_ORDER
Большая арпа (100_000 сабвордов) с маленьким прунингом для вот такой [конфигурации](../configs/fourgram_bigsub_config.json) не собралась. Нужно перегенерировать арпу и пересобрать модель.
Ветка в репозитории `feature/four_grams_subs_testing`
закоментировать 67 строчку cleanup скрипта lm_build.sh, если потребуется отладка.
- [x] Распараллелить циклы прямого и обратного прохода.
- [ ] `g2p` без `_Е` на уровне Python скриптов.
- [ ] Убрать сохранение файла `.priors` (возможно в dnn_proc)


## Задачи по сабвордам.
- [x] Составить стату по сабвордам в процентах. Сколько процентов односложных,
сколько двусложных и т.д.
- [x] Сделать сабворды `--prune 4 4 5`, удостовериться, что модель адекватная.


Решено сделать несколько конфигов и выходных файлов:
| Число сабов | расширение | lm_corpus | lm | dict |
|---|---|---|---|---|
| 5_000 | .quantumsub | **complete** | **quantumsub_work** | encanced_quantumsub.dic |
| 15_000 | .nanosub |  **complete** | no | |
| 20_000 | .microsub | no | no | |
| 50_000 | .minisub | **complete** | **minisub_work** | enhanced_minisub.dic |
| 70_000 | .regsub | **complete** | no | |
| 100_000 | .bigsub | **complete** | **bigsub_work** | enhanced_bigsub.dic |
| 200_000 | .titansub | **complete** | no |
| 300_000 | .colossub | **complete** | **colossub_work** | enhanced_colossub.dic
Конфигурационный файл один и тотже, но разные расширения `maga_news.conf`.

## Маленькие задачи
- [x] Проверить каскадное обучение  ru/ru на 100 часовом корпусе, 15 эпох.  
- [x] Сделать `config.json` консольным параметром `run.py`  
- [x] Добавить возможность проброски стадий из консоли параметром.
- [x] Алгоритм поиска лучшей сетки не учитывает смену даты. Latest указывает не туда
и выбирается не та сетка (numpy-файл не тот при обучении в течении нескольких суток)

## Большие задачи
- [ ] Прикрутить `tensor_board`  
- [ ] Сервер на `flask`, логи и пр. в `SQL-Alchemy`, клиент на модуле `dashboard`  
- [ ] Запустить `lstm` на `tensorflow-lite`  
- [ ] Продолжить `subwords`  
- [ ] Продолжить `g2p` с `attention`  
